{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"User Churn Prediction \u00b6 This project develops a predictive model to predict customer/user churn. The pipeline includes data preprocessing, exploratory data analysis, hyperparameter tuning, model selection and model interpretation parts. The code in this project is commited by Utku Can Ozturk .","title":"Home"},{"location":"#user-churn-prediction","text":"This project develops a predictive model to predict customer/user churn. The pipeline includes data preprocessing, exploratory data analysis, hyperparameter tuning, model selection and model interpretation parts. The code in this project is commited by Utku Can Ozturk .","title":"User Churn Prediction"},{"location":"data/","text":"Dataset \u00b6 Social media platforms often look at when users churn. In order to analyze the user churn we will use a dataset of user events. The dataset contains information about the users, i.e., the events they performed, the count of each event, as well as the target variable indicating retention score of the user. The lower the retention score, the higher the probability of users churning. The dataset includes data from 373296 users and 52 user activities, features. Features include activities like comment liked, comment posted, screen view. First Look \u00b6 The dataset does not include any missing feature values. We see positive skewnes in the target variable and feature distributions. Therefore, we can say that none of the columns in the dataset follow normal distribution. To analyze it further, skewness of the features and the target variable is visualized below. Skewness of the features and the target variable As you can see from the graph above, all of the variables are right-skewed and some of them have significant skewness in their distribution. Positive skewness is the consequence of large number of zeros in the feature columns. Since our features represent the number of event occurences for a user, we would expect to have many zeros in the columns. One step further, the percentage of non-zero values for each feature column is visualized in the plot below. Non-zero Percentage of the features By looking at the column non-zero percentages, we can see that screen_view has the least non-zero rows with 25% of user database having this activity. screen_view is followed by first_open , item_added and item_removed whose percentages lie between 10 and 20. Furthermore, for some features, events, users have little to none activity such as user_blocked , firebase_in_app_message_action , firebase_in_app_message_dismiss and 22 other features has less than 1% nonzero values that represent user acitivity. Therefore, our inference from this values is: some variables have slight to none variability which might indicate less information gain on those variables; this will be taking into account in the feature selection stage of our model tuning stage. Correlations \u00b6 Analyzing correlations between variable pairs is a basic but significant part of our dataset analysis. Therefore, the correlations are visualized in a correlation heatmap in the figure below. It can be seen that almost all pairs of variables have an absolute correlation coefficient between 0 and 0.5, indicating a low or negligible correlation. Also, only strongly correlated pair in the dataset seem to be the retentionScore-item_added pair which can be seen in the dark red color in the heatmap. Other than that, there are little to no correlations among the features meaning no multicollinearity exist in the dataset. Correlations Heatmap Now let us focus on the feature-target variable pair correlations. Feature-target variable, retentionScore , correlations are analyzed and ten of those having highest absolute correlation coefficient are visualized below. Top 10 Predictor Correlations with the Target Variable In the graph above, you can see that event item_added is the highest correlated feature with retentionScore with positive correlation coefficient of 0.94. We also see that outfit_liked has the second highest correlation with the target variable with positive correlation coefficient of 0.34. Moreover, we see low positive correlation with events like comment_liked , comment_posted , and low negative correlation with events like app_remove . Later, we will analyze features' impacts on the prediction when proper machine learning models are used. Analysis of Feature - Target Patterns \u00b6 The correlations of features and the target variable gave us the first insight to the feature-target relationships. Now, we will analyze those relationships furher by focusing on the first five most correlated features. To do that, we would like to have less-skewed and equal-spreaded data. Also, since we are interested in retetion score of the user it would be useful to have a look at the target variable from narrower scale. Therefore, the target variable is log-transformed and rounded into categories. Discrete log-retention score distributions of the users are shown below. # of Users in the Retention Category We want to see value distributions of the features over the discretized retention scores of the users are differ from each other. To analyze that, firstly, mean event counts for each retention category is visualized below: Average event counts grouped by retention category As we can see from the figure above, event counts for the lower retention categories, meaning lower retention scores, are close to zero compare to higher scores. Therefore, we see positive relationship between these event counts and the retention scores. However, as we saw from the target value counts plot above, sample sizes between categories can be significantly different, and zero values in the categories might be misleading us in the large sized categories. Therefore, currently no inference can be made; features needs to be further analyzed in the model evaluation stage. Next, boxplot of the event counts for each retention category is visualized below. By looking at the plot, we see that outliers are mostly exist in the higher retention scores except item_removed . Moreover, for item_added we see increasing medians with retention scores. Boxplots of event counts grouped by retention category Boxplot of item_added counts grouped by retention category Finally, item_added versus retentionScore scatter with regression line is plotted and visualized below. From there we see a regression line with positive slope fitted between the variables. item_added vs. retentionScore scatter plot with fitted regression line Now, that we have a initial understanding of the dataset, we want to first build machine learning models, which can predict user retention scores as precise as possible (see section Modeling ) and then evaluate their performance.","title":"Dataset"},{"location":"data/#dataset","text":"Social media platforms often look at when users churn. In order to analyze the user churn we will use a dataset of user events. The dataset contains information about the users, i.e., the events they performed, the count of each event, as well as the target variable indicating retention score of the user. The lower the retention score, the higher the probability of users churning. The dataset includes data from 373296 users and 52 user activities, features. Features include activities like comment liked, comment posted, screen view.","title":"Dataset"},{"location":"data/#first-look","text":"The dataset does not include any missing feature values. We see positive skewnes in the target variable and feature distributions. Therefore, we can say that none of the columns in the dataset follow normal distribution. To analyze it further, skewness of the features and the target variable is visualized below. Skewness of the features and the target variable As you can see from the graph above, all of the variables are right-skewed and some of them have significant skewness in their distribution. Positive skewness is the consequence of large number of zeros in the feature columns. Since our features represent the number of event occurences for a user, we would expect to have many zeros in the columns. One step further, the percentage of non-zero values for each feature column is visualized in the plot below. Non-zero Percentage of the features By looking at the column non-zero percentages, we can see that screen_view has the least non-zero rows with 25% of user database having this activity. screen_view is followed by first_open , item_added and item_removed whose percentages lie between 10 and 20. Furthermore, for some features, events, users have little to none activity such as user_blocked , firebase_in_app_message_action , firebase_in_app_message_dismiss and 22 other features has less than 1% nonzero values that represent user acitivity. Therefore, our inference from this values is: some variables have slight to none variability which might indicate less information gain on those variables; this will be taking into account in the feature selection stage of our model tuning stage.","title":"First Look"},{"location":"data/#correlations","text":"Analyzing correlations between variable pairs is a basic but significant part of our dataset analysis. Therefore, the correlations are visualized in a correlation heatmap in the figure below. It can be seen that almost all pairs of variables have an absolute correlation coefficient between 0 and 0.5, indicating a low or negligible correlation. Also, only strongly correlated pair in the dataset seem to be the retentionScore-item_added pair which can be seen in the dark red color in the heatmap. Other than that, there are little to no correlations among the features meaning no multicollinearity exist in the dataset. Correlations Heatmap Now let us focus on the feature-target variable pair correlations. Feature-target variable, retentionScore , correlations are analyzed and ten of those having highest absolute correlation coefficient are visualized below. Top 10 Predictor Correlations with the Target Variable In the graph above, you can see that event item_added is the highest correlated feature with retentionScore with positive correlation coefficient of 0.94. We also see that outfit_liked has the second highest correlation with the target variable with positive correlation coefficient of 0.34. Moreover, we see low positive correlation with events like comment_liked , comment_posted , and low negative correlation with events like app_remove . Later, we will analyze features' impacts on the prediction when proper machine learning models are used.","title":"Correlations"},{"location":"data/#analysis-of-feature-target-patterns","text":"The correlations of features and the target variable gave us the first insight to the feature-target relationships. Now, we will analyze those relationships furher by focusing on the first five most correlated features. To do that, we would like to have less-skewed and equal-spreaded data. Also, since we are interested in retetion score of the user it would be useful to have a look at the target variable from narrower scale. Therefore, the target variable is log-transformed and rounded into categories. Discrete log-retention score distributions of the users are shown below. # of Users in the Retention Category We want to see value distributions of the features over the discretized retention scores of the users are differ from each other. To analyze that, firstly, mean event counts for each retention category is visualized below: Average event counts grouped by retention category As we can see from the figure above, event counts for the lower retention categories, meaning lower retention scores, are close to zero compare to higher scores. Therefore, we see positive relationship between these event counts and the retention scores. However, as we saw from the target value counts plot above, sample sizes between categories can be significantly different, and zero values in the categories might be misleading us in the large sized categories. Therefore, currently no inference can be made; features needs to be further analyzed in the model evaluation stage. Next, boxplot of the event counts for each retention category is visualized below. By looking at the plot, we see that outliers are mostly exist in the higher retention scores except item_removed . Moreover, for item_added we see increasing medians with retention scores. Boxplots of event counts grouped by retention category Boxplot of item_added counts grouped by retention category Finally, item_added versus retentionScore scatter with regression line is plotted and visualized below. From there we see a regression line with positive slope fitted between the variables. item_added vs. retentionScore scatter plot with fitted regression line Now, that we have a initial understanding of the dataset, we want to first build machine learning models, which can predict user retention scores as precise as possible (see section Modeling ) and then evaluate their performance.","title":"Analysis of Feature - Target Patterns"},{"location":"extra/","text":"Extras \u00b6 New User Share \u00b6 We have the following data table: id int time_id datetime user_id varchar customer_id varchar client_id varchar event_type varchar event_id int We want to calculate the share of new and existing users and output the month, share of new users, and share of existing users as a ratio. Here, new user refers to the that started using the product within current month. Two alternative queries are proposed to do this calculation and you can find both of them below. Alternative Query 1 \u00b6 DELIMITER // CREATE PROCEDURE GetNewUserShare () BEGIN DECLARE new_users INT ; DECLARE users INT ; DECLARE cur_month INT ; SELECT MONTH ( current_date ) INTO cur_month ; SELECT COUNT ( DISTINCT user_id ) INTO new_users FROM Test WHERE MONTH ( time_id ) = cur_month ; SELECT COUNT ( DISTINCT user_id ) INTO users FROM Test ; CREATE TEMPORARY TABLE t ( current_ month INT NULL , new_user_share FLOAT NULL , existing_user_share FLOAT NULL ); INSERT INTO t VALUES ( cur_month , new_users / ( users ), ( users - new_users ) / users ); SELECT * FROM t ; END // DELIMITER ; CALL GetNewUserShare (); Alternative Query 2 \u00b6 DELIMITER // CREATE PROCEDURE GetNewUserShare2 () BEGIN SELECT MONTH ( current_date ) AS current_month , SUM ( CASE WHEN MONTH ( time_id ) = MONTH ( current_date ) THEN 1 ELSE 0 END ) / COUNT ( * ) AS new_user_ratio , SUM ( CASE WHEN MONTH ( time_id ) = MONTH ( current_date ) THEN 0 ELSE 1 END ) / COUNT ( * ) AS existing_user_ratio FROM Test ; END // DELIMITER ; CALL GetNewUserShare2 (); Both of the queries outputs the same table represented below. GetNewUserShare() current_month int new_user_ratio float existing_user_ratio float nth Highest Follower Count \u00b6 We have the following followerCount table: Id followerCount 1 100 2 200 3 300 Following query returns the instance with nth highest follower count on the table: DELIMITER // CREATE PROCEDURE NthHighestFollowerCount ( IN n INT ) BEGIN DECLARE n2 INT DEFAULT n - 1 ; SELECT followerCount FROM followerCount ORDER BY followerCount DESC LIMIT n2 , 1 ; END // DELIMITER ; CALL NthHighestFollowerCount ( 2 ) Query output will be as follows: NthHighestFollowerCount (2) 200 Dynamic Feed \u00b6 Let say there is a social media app that currently has a static content feed (similar to Instagram\u2019s explore page but the content is the same for everyone). Currently, the feed is static where it shows the most liked outfits from the last 12-hour window. We want to devise a plan to make this feed fully dynamic such that the user\u2019s previous activity and geographically trending outfits are shown. What can we do? To do that first we need to identify what would be helpful for our analysis: User statistics This data will be representing the individual actions of the user (e.g. liked, saved, comments liked, comments posted). Finding similar accounts Accounts people have interacted with before is crucial for our dynamic feed. We would find better content for a user if we can identify other accounts a person might be interested in. We can use this interacted accounts to find media these accounts have posted or engaged with. This interactions might be occur in several ways like follow, like, comment, save, share etc. Using spatial data Spatial data useful while improving our similar account pool meaning we can use geo-distance as a measure to similarity to the other accounts. Similar accounts for the user might result in very high number of people that we cannot fit to our feed. In that case, we hve to implement ranking algorithm to narrow down those choices to fit our feed. How we can do this ranking? We can create similarity-dissimilarity metrics with individual statistics and similar account statistics. However, those statistics needs to be weighted and therefore tuned on past data. We can use algorithms like KDTree (usually runs very fast even with the large amount of data) to rank our similar accounts. We can integrate kmeans clustering for the contents. Hence, instead of ranking the accounts we can rank the related posts. After all we don't want to end up with a feed with very similar posts. Therefore, we should implement categorization to maintain diversity in the content feed. This can be done by labeling posts in our database.","title":"Extras"},{"location":"extra/#extras","text":"","title":"Extras"},{"location":"extra/#new-user-share","text":"We have the following data table: id int time_id datetime user_id varchar customer_id varchar client_id varchar event_type varchar event_id int We want to calculate the share of new and existing users and output the month, share of new users, and share of existing users as a ratio. Here, new user refers to the that started using the product within current month. Two alternative queries are proposed to do this calculation and you can find both of them below.","title":"New User Share"},{"location":"extra/#alternative-query-1","text":"DELIMITER // CREATE PROCEDURE GetNewUserShare () BEGIN DECLARE new_users INT ; DECLARE users INT ; DECLARE cur_month INT ; SELECT MONTH ( current_date ) INTO cur_month ; SELECT COUNT ( DISTINCT user_id ) INTO new_users FROM Test WHERE MONTH ( time_id ) = cur_month ; SELECT COUNT ( DISTINCT user_id ) INTO users FROM Test ; CREATE TEMPORARY TABLE t ( current_ month INT NULL , new_user_share FLOAT NULL , existing_user_share FLOAT NULL ); INSERT INTO t VALUES ( cur_month , new_users / ( users ), ( users - new_users ) / users ); SELECT * FROM t ; END // DELIMITER ; CALL GetNewUserShare ();","title":"Alternative Query 1"},{"location":"extra/#alternative-query-2","text":"DELIMITER // CREATE PROCEDURE GetNewUserShare2 () BEGIN SELECT MONTH ( current_date ) AS current_month , SUM ( CASE WHEN MONTH ( time_id ) = MONTH ( current_date ) THEN 1 ELSE 0 END ) / COUNT ( * ) AS new_user_ratio , SUM ( CASE WHEN MONTH ( time_id ) = MONTH ( current_date ) THEN 0 ELSE 1 END ) / COUNT ( * ) AS existing_user_ratio FROM Test ; END // DELIMITER ; CALL GetNewUserShare2 (); Both of the queries outputs the same table represented below. GetNewUserShare() current_month int new_user_ratio float existing_user_ratio float","title":"Alternative Query 2"},{"location":"extra/#nth-highest-follower-count","text":"We have the following followerCount table: Id followerCount 1 100 2 200 3 300 Following query returns the instance with nth highest follower count on the table: DELIMITER // CREATE PROCEDURE NthHighestFollowerCount ( IN n INT ) BEGIN DECLARE n2 INT DEFAULT n - 1 ; SELECT followerCount FROM followerCount ORDER BY followerCount DESC LIMIT n2 , 1 ; END // DELIMITER ; CALL NthHighestFollowerCount ( 2 ) Query output will be as follows: NthHighestFollowerCount (2) 200","title":"nth Highest Follower Count"},{"location":"extra/#dynamic-feed","text":"Let say there is a social media app that currently has a static content feed (similar to Instagram\u2019s explore page but the content is the same for everyone). Currently, the feed is static where it shows the most liked outfits from the last 12-hour window. We want to devise a plan to make this feed fully dynamic such that the user\u2019s previous activity and geographically trending outfits are shown. What can we do? To do that first we need to identify what would be helpful for our analysis: User statistics This data will be representing the individual actions of the user (e.g. liked, saved, comments liked, comments posted). Finding similar accounts Accounts people have interacted with before is crucial for our dynamic feed. We would find better content for a user if we can identify other accounts a person might be interested in. We can use this interacted accounts to find media these accounts have posted or engaged with. This interactions might be occur in several ways like follow, like, comment, save, share etc. Using spatial data Spatial data useful while improving our similar account pool meaning we can use geo-distance as a measure to similarity to the other accounts. Similar accounts for the user might result in very high number of people that we cannot fit to our feed. In that case, we hve to implement ranking algorithm to narrow down those choices to fit our feed. How we can do this ranking? We can create similarity-dissimilarity metrics with individual statistics and similar account statistics. However, those statistics needs to be weighted and therefore tuned on past data. We can use algorithms like KDTree (usually runs very fast even with the large amount of data) to rank our similar accounts. We can integrate kmeans clustering for the contents. Hence, instead of ranking the accounts we can rank the related posts. After all we don't want to end up with a feed with very similar posts. Therefore, we should implement categorization to maintain diversity in the content feed. This can be done by labeling posts in our database.","title":"Dynamic Feed"},{"location":"model/","text":"Modeling \u00b6 Since it is unknown what are the measures for the user retention, various machine learning algorithms are compared in terms of their ability to distinguish user retention score. To be able to assess the quality of each model a 3-fold cross-validation procedure is performed. Finally the evaluation of promising algorithms will be done in Model Evaluation . Evaluation Metric \u00b6 Probably the most straight forward metric to evaluate regression tasks is root mean squared error (RMSE) and in this regression analysis we will use RMSE as our evaluation metric. RMSE is the square root of the average of squared errors. The RMSE serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. Algorithms \u00b6 We will use two gradient boosting frameworks in our modeling approach which are Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM). Both algorithms working by growing boosted decision trees. However, there is slight difference in the growing method. While XGBoost applies level-wise tree growth LightGBM applies leaf-wise tree growth. Level-wise approach meaning it grows horizontal whereas leaf-wise grows vertical. Moreover, leaf-wise approach is mostly faster than the level-wise approach making LightGBM appealing. However, leaf-wise more likely to overfit on the data, therefore we can say XGBoost has the advantage of building more robust models. Ligth Gradient Boosting Machine (LightGBM) \u00b6 For the LightGBM parameters number of estimators (n_estimators), maximum tree depth (max_depth) and learning rate (learning_rate) is tuned prior to the benchmark in a 3-fold cross validation. Also, feature selector that removes low-variance features is tuned with different thresholds of 0, 0.01 and 0.001. Moreover, all the features are scaled to a standard scale using standard-scaler . By using the data twice, once for hyperparameter tuning and once for the benchmark, we might underestimate the generalization error. However since hyperparameter tuning is computationally expensive, and we are not interested in the precise generalization error of each model but rather the general tendency of each model's performance, we refrained from nested resampling. After the tuning, following parameter set resulted with the best mean test set score: Parameter Lower Bound Upper Bound RMSE Optimal Value variance_threshold 0.001 0 0 learning_rate 0.01 0.2 0.2 max_depth -1 20 -1 n_estimators 50 500 500 Our best model's test set score averaged at ~0.88 for the LightGBM. For further analysis, mean test score versus parameters plot can be seen below. LightGBM Parameters vs. Mean Test Score Extreme Gradient Boosting (XGBoost) \u00b6 For the XGBoost parameters number of estimators (n_estimators), maximum tree depth (max_depth) and learning rate (eta) is tuned prior to the benchmark in a 3-fold cross validation. Here, standard-scaler used but feature selector hasn't applied since it showed no difference in the previous analysis. After the tuning, following parameter set resulted with the best mean test set score: Parameter Lower Bound Upper Bound RMSE Optimal Value learning_rate 0.05 0.2 0.05 max_depth 3 12 3 n_estimators 100 500 300 Our best model's test set score averaged at ~0.95 for the XGBoost. For further analysis, mean test score versus parameters plot can be seen below. XGBoost Parameters vs. Mean Test Score Support Vector Machines (SVM) \u00b6 Support vector machines (SVM) try to find a separating hyperplane between the target categories. In this analysis, a radial kernel is recommended and the hyperparameters cost and gamma should be tuned in cross-validation prior to the benchmark. It would be nice to have a kernel base model trained on the dataset however it requires much more power and time to train SVMs. Therefore, we will stick to the tree-based methods which seems to have good performance on our dataset. Model Evaluation \u00b6 Since we now know the best parameters for our models. Models trained on training set with those parameters then metrics are calculated over the whole dataset. Error metrics for the both models are tabulated below: Model MSE RMSE R2 MAE MAD XGBoost 55.547 7.453 0.997 0.420 0.192 LightGBM 1271.374 35.656 0.924 0.842 0.061 By looking at the metrics above, we can say that the XGBoost outperformed the LightGBM regressor. However, we should analyze the residuals to be sure about our decision. First we would like to check whether the residuals are centered around zero or not. To do that 95% confidence interval is constructed around the residual means. For XGBoost 95% CI corresponds to (-0.010, 0.038) and for LightGBM it is (-0.202, 0.026) . For both predictors' confidence intervals covers zero then we can say that our predictions are unbiased. Next we will be looking at the scatter plot of the residuals of the model. From the figure below, you can see the residual distribution of the predictions which fall into 99.9th percentile. Scatter plot of the residuals within 99.9th percentile From the above plot, residual distributions seem to be randomly distributed with mean close to zero. Reverse cumulative distribution of the residuals is show below in order to further asses our predictor performances. Reverse cumulative distribution of the residuals of the models are proof for the model performances. As you can see from the plot, ~90% of the residuals are fall into the range between 0 and 1 indicating little to no errors. We see steeper decrease of reverse cumulative function for the LightGBM however XGBoost covers it up in the higher residual values. Therefore, LightGBM might be a better fit for more than 80% of the data while XGBoost is more robust over the dataset. Reverse cumulative distribution of the residuals Lastly, we want to compare predictions and actual scores for XGBoost model in the plot below. Here, we don't see many points far from the diagonal which is a good sign. Green points represents the errors with less than 5% mean absolute percentage and we see a lot of green points when we zoomed into the bottom-left corner of the plot. Comparison between predictions and actual scores for XGBoost Comparison between predictions and actual scores for XGBoost bottom-left zoomed Feature Importances \u00b6 Permutation Feature Importance \u00b6 Permutation feature importance is, as the name suggests, a method to quantify the importance of a feature. It is measured by calculating the increase in the model\u2019s prediction error after permuting the feature of interest. Intuitively, permuting or shuffling all values of a feature destroys any relationship between the given feature and the target. If the error increases by breaking the relationship, the given feature must have been important for model prediction. It is important to analyze which features are models mostly depending on. Therefore, feature importance plots for XGBoost and LightGBm are shown below: Feature impotances for XGBoost Feature importances for LightGBM When we look at the feature importance plots, we can see that top 4 important features and their order are the same for both models. Those features are outfit_liked , item_added , comment_posted and comment_liked with significant contribution compare to the other features. Shapley Feature Importance \u00b6 Shapley values help us to measure each feature\u2019s contribution to the prediction of an instance, more precisely they measure how much a feature contributed to the prediction compared to the average prediction. They are calculated by averaging the marginal payout or contribution for each feature. Assuming we are interested in the contribution of feature x. First, the prediction for every combination of features not including x is calculated. Then we measure the marginal contribution of x, i.e. how adding x to each combination changes the prediction. Finally, all marginal contributions are averaged and we obtain the Shapley value for feature x. Since there are many possible coalitions of the feature values, calculating Shapley values is computationally expensive. We want to asses whether the feature importance is also consistent when using a Shapley based measure instead of permutation feature importance. For each feature, the Shapley feature importance is the mean absolute value of the Shapley values of all observations. This means that features which have high (absolute) Shapley values for many emails have a high importance in general. Shapley values of XGBoost model can be found in the figure below. By looking at the mean shapley values of the features, we can say that permutation and shapley plots are consistent with each other on the top 4 features. Shapley feature impotances for XGBoost Statistical Models \u00b6 Machine learning techniques provided good results on the retention score prediction. Now, we would like to see how statistical models are performing on the user data. For that purpose we will use generalized linear models (GLMs). The GLM regression finds the most likely value of variable coefficients (\u00df) for the given data. In most cases this requires an iterative algorithm that maximises the log-likelihood function. In our case, since our data is right-skewed it needs prior transformation to fit the regression models on otherwise model assumptions will be violated. Response functions that could be useful to model right-skewed outcome are poisson and negative binomial. However, those family functions require discrete response variable. In our case, the target variable that we have is continuous, positive and right-skewed. Therfore, we will use Gaussian and Gamma families and apply log-transformation on the dataset. As our initial predictors, we will use the top 4 features from our machine learning models which are outfit_liked , item_added , comment_posted and comment_liked . Also, we will not include any interaction terms for now. Gaussian model fit Above plot shows the fit for retention score by the gaussian model. As we go towards higher retention scores in the plot, the prediction errors seem to go larger. Therefore, residuals versus predictions are visualized below. From there, gaussian model overpredicts in the higher range of retention scores since we see larger negative errors. Gaussian model residuals To eliminate the tendency of overprediction, interaction terms are decided to be added to the model. Resulting model fit is visualized below. Gaussian model with interactions fit The model with interactions has much smaller deviance and pearson score indicating better fit to our data. Then, gamma model applied but it wasn't a good fit to the data. We stopped our anaylsis at this point. However, models with different link functions or penalties might be introduced to find a better fitting model. Also, different approcahes like ridge or lasso regression can be implemented with different link functions.","title":"Modeling"},{"location":"model/#modeling","text":"Since it is unknown what are the measures for the user retention, various machine learning algorithms are compared in terms of their ability to distinguish user retention score. To be able to assess the quality of each model a 3-fold cross-validation procedure is performed. Finally the evaluation of promising algorithms will be done in Model Evaluation .","title":"Modeling"},{"location":"model/#evaluation-metric","text":"Probably the most straight forward metric to evaluate regression tasks is root mean squared error (RMSE) and in this regression analysis we will use RMSE as our evaluation metric. RMSE is the square root of the average of squared errors. The RMSE serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power.","title":"Evaluation Metric"},{"location":"model/#algorithms","text":"We will use two gradient boosting frameworks in our modeling approach which are Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM). Both algorithms working by growing boosted decision trees. However, there is slight difference in the growing method. While XGBoost applies level-wise tree growth LightGBM applies leaf-wise tree growth. Level-wise approach meaning it grows horizontal whereas leaf-wise grows vertical. Moreover, leaf-wise approach is mostly faster than the level-wise approach making LightGBM appealing. However, leaf-wise more likely to overfit on the data, therefore we can say XGBoost has the advantage of building more robust models.","title":"Algorithms"},{"location":"model/#ligth-gradient-boosting-machine-lightgbm","text":"For the LightGBM parameters number of estimators (n_estimators), maximum tree depth (max_depth) and learning rate (learning_rate) is tuned prior to the benchmark in a 3-fold cross validation. Also, feature selector that removes low-variance features is tuned with different thresholds of 0, 0.01 and 0.001. Moreover, all the features are scaled to a standard scale using standard-scaler . By using the data twice, once for hyperparameter tuning and once for the benchmark, we might underestimate the generalization error. However since hyperparameter tuning is computationally expensive, and we are not interested in the precise generalization error of each model but rather the general tendency of each model's performance, we refrained from nested resampling. After the tuning, following parameter set resulted with the best mean test set score: Parameter Lower Bound Upper Bound RMSE Optimal Value variance_threshold 0.001 0 0 learning_rate 0.01 0.2 0.2 max_depth -1 20 -1 n_estimators 50 500 500 Our best model's test set score averaged at ~0.88 for the LightGBM. For further analysis, mean test score versus parameters plot can be seen below. LightGBM Parameters vs. Mean Test Score","title":"Ligth Gradient Boosting Machine (LightGBM)"},{"location":"model/#extreme-gradient-boosting-xgboost","text":"For the XGBoost parameters number of estimators (n_estimators), maximum tree depth (max_depth) and learning rate (eta) is tuned prior to the benchmark in a 3-fold cross validation. Here, standard-scaler used but feature selector hasn't applied since it showed no difference in the previous analysis. After the tuning, following parameter set resulted with the best mean test set score: Parameter Lower Bound Upper Bound RMSE Optimal Value learning_rate 0.05 0.2 0.05 max_depth 3 12 3 n_estimators 100 500 300 Our best model's test set score averaged at ~0.95 for the XGBoost. For further analysis, mean test score versus parameters plot can be seen below. XGBoost Parameters vs. Mean Test Score","title":"Extreme Gradient Boosting (XGBoost)"},{"location":"model/#support-vector-machines-svm","text":"Support vector machines (SVM) try to find a separating hyperplane between the target categories. In this analysis, a radial kernel is recommended and the hyperparameters cost and gamma should be tuned in cross-validation prior to the benchmark. It would be nice to have a kernel base model trained on the dataset however it requires much more power and time to train SVMs. Therefore, we will stick to the tree-based methods which seems to have good performance on our dataset.","title":"Support Vector Machines (SVM)"},{"location":"model/#model-evaluation","text":"Since we now know the best parameters for our models. Models trained on training set with those parameters then metrics are calculated over the whole dataset. Error metrics for the both models are tabulated below: Model MSE RMSE R2 MAE MAD XGBoost 55.547 7.453 0.997 0.420 0.192 LightGBM 1271.374 35.656 0.924 0.842 0.061 By looking at the metrics above, we can say that the XGBoost outperformed the LightGBM regressor. However, we should analyze the residuals to be sure about our decision. First we would like to check whether the residuals are centered around zero or not. To do that 95% confidence interval is constructed around the residual means. For XGBoost 95% CI corresponds to (-0.010, 0.038) and for LightGBM it is (-0.202, 0.026) . For both predictors' confidence intervals covers zero then we can say that our predictions are unbiased. Next we will be looking at the scatter plot of the residuals of the model. From the figure below, you can see the residual distribution of the predictions which fall into 99.9th percentile. Scatter plot of the residuals within 99.9th percentile From the above plot, residual distributions seem to be randomly distributed with mean close to zero. Reverse cumulative distribution of the residuals is show below in order to further asses our predictor performances. Reverse cumulative distribution of the residuals of the models are proof for the model performances. As you can see from the plot, ~90% of the residuals are fall into the range between 0 and 1 indicating little to no errors. We see steeper decrease of reverse cumulative function for the LightGBM however XGBoost covers it up in the higher residual values. Therefore, LightGBM might be a better fit for more than 80% of the data while XGBoost is more robust over the dataset. Reverse cumulative distribution of the residuals Lastly, we want to compare predictions and actual scores for XGBoost model in the plot below. Here, we don't see many points far from the diagonal which is a good sign. Green points represents the errors with less than 5% mean absolute percentage and we see a lot of green points when we zoomed into the bottom-left corner of the plot. Comparison between predictions and actual scores for XGBoost Comparison between predictions and actual scores for XGBoost bottom-left zoomed","title":"Model Evaluation"},{"location":"model/#feature-importances","text":"","title":"Feature Importances"},{"location":"model/#permutation-feature-importance","text":"Permutation feature importance is, as the name suggests, a method to quantify the importance of a feature. It is measured by calculating the increase in the model\u2019s prediction error after permuting the feature of interest. Intuitively, permuting or shuffling all values of a feature destroys any relationship between the given feature and the target. If the error increases by breaking the relationship, the given feature must have been important for model prediction. It is important to analyze which features are models mostly depending on. Therefore, feature importance plots for XGBoost and LightGBm are shown below: Feature impotances for XGBoost Feature importances for LightGBM When we look at the feature importance plots, we can see that top 4 important features and their order are the same for both models. Those features are outfit_liked , item_added , comment_posted and comment_liked with significant contribution compare to the other features.","title":"Permutation Feature Importance"},{"location":"model/#shapley-feature-importance","text":"Shapley values help us to measure each feature\u2019s contribution to the prediction of an instance, more precisely they measure how much a feature contributed to the prediction compared to the average prediction. They are calculated by averaging the marginal payout or contribution for each feature. Assuming we are interested in the contribution of feature x. First, the prediction for every combination of features not including x is calculated. Then we measure the marginal contribution of x, i.e. how adding x to each combination changes the prediction. Finally, all marginal contributions are averaged and we obtain the Shapley value for feature x. Since there are many possible coalitions of the feature values, calculating Shapley values is computationally expensive. We want to asses whether the feature importance is also consistent when using a Shapley based measure instead of permutation feature importance. For each feature, the Shapley feature importance is the mean absolute value of the Shapley values of all observations. This means that features which have high (absolute) Shapley values for many emails have a high importance in general. Shapley values of XGBoost model can be found in the figure below. By looking at the mean shapley values of the features, we can say that permutation and shapley plots are consistent with each other on the top 4 features. Shapley feature impotances for XGBoost","title":"Shapley Feature Importance"},{"location":"model/#statistical-models","text":"Machine learning techniques provided good results on the retention score prediction. Now, we would like to see how statistical models are performing on the user data. For that purpose we will use generalized linear models (GLMs). The GLM regression finds the most likely value of variable coefficients (\u00df) for the given data. In most cases this requires an iterative algorithm that maximises the log-likelihood function. In our case, since our data is right-skewed it needs prior transformation to fit the regression models on otherwise model assumptions will be violated. Response functions that could be useful to model right-skewed outcome are poisson and negative binomial. However, those family functions require discrete response variable. In our case, the target variable that we have is continuous, positive and right-skewed. Therfore, we will use Gaussian and Gamma families and apply log-transformation on the dataset. As our initial predictors, we will use the top 4 features from our machine learning models which are outfit_liked , item_added , comment_posted and comment_liked . Also, we will not include any interaction terms for now. Gaussian model fit Above plot shows the fit for retention score by the gaussian model. As we go towards higher retention scores in the plot, the prediction errors seem to go larger. Therefore, residuals versus predictions are visualized below. From there, gaussian model overpredicts in the higher range of retention scores since we see larger negative errors. Gaussian model residuals To eliminate the tendency of overprediction, interaction terms are decided to be added to the model. Resulting model fit is visualized below. Gaussian model with interactions fit The model with interactions has much smaller deviance and pearson score indicating better fit to our data. Then, gamma model applied but it wasn't a good fit to the data. We stopped our anaylsis at this point. However, models with different link functions or penalties might be introduced to find a better fitting model. Also, different approcahes like ridge or lasso regression can be implemented with different link functions.","title":"Statistical Models"}]}